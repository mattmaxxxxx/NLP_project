{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment classifier with LSTM\n\nIn this notebook, we will implement a simple sentiment classifier using an LSTM. Follow the notebook, complete the missing part, answer the questions and apply the asked modifications.\n\n","metadata":{"id":"1DlzvXLQuoTY"}},{"cell_type":"code","source":"!pip install datasets","metadata":{"id":"utTvPhcyTC7d","outputId":"d5126fe6-ba58-41a9-b674-592bd64fdedd","execution":{"iopub.status.busy":"2022-10-06T21:22:32.617051Z","iopub.execute_input":"2022-10-06T21:22:32.617636Z","iopub.status.idle":"2022-10-06T21:22:44.150772Z","shell.execute_reply.started":"2022-10-06T21:22:32.617503Z","shell.execute_reply":"2022-10-06T21:22:44.149616Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.3.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\nfrom copy import deepcopy\nimport re\nfrom typing import Callable, List, Tuple\n\nfrom datasets import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom sklearn.utils import shuffle","metadata":{"id":"g_v62rH_W9J1","execution":{"iopub.status.busy":"2022-10-06T21:22:44.154421Z","iopub.execute_input":"2022-10-06T21:22:44.154795Z","iopub.status.idle":"2022-10-06T21:22:46.885355Z","shell.execute_reply.started":"2022-10-06T21:22:44.154762Z","shell.execute_reply":"2022-10-06T21:22:46.884293Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Downloading the dataset\n\nUsing the datasets library, we load the imdb dataset.","metadata":{"id":"FzAHrPfReDqu"}},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")","metadata":{"id":"PBb40sTOXYz-","outputId":"80e7c713-a937-4940-ae6e-e5670d057ef7","execution":{"iopub.status.busy":"2022-10-06T21:22:46.889432Z","iopub.execute_input":"2022-10-06T21:22:46.889950Z","iopub.status.idle":"2022-10-06T21:23:17.150631Z","shell.execute_reply.started":"2022-10-06T21:22:46.889919Z","shell.execute_reply":"2022-10-06T21:23:17.149662Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c098f785f154152a8a0f06b582063b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc6d1629eb74697a59f8c80527d242a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"592fe7c2198f4642980cf8f37fddc5f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7fe52e456f4ad7bb5bed1e76b28b21"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"id":"uDq8WhU3XlOR","outputId":"43c27a4a-ecc3-4c49-950c-34fcf0f131ca","execution":{"iopub.status.busy":"2022-10-06T21:23:17.152071Z","iopub.execute_input":"2022-10-06T21:23:17.152912Z","iopub.status.idle":"2022-10-06T21:23:17.162525Z","shell.execute_reply.started":"2022-10-06T21:23:17.152872Z","shell.execute_reply":"2022-10-06T21:23:17.161344Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# We do not need the \"unsupervised\" split.\ndataset.pop(\"unsupervised\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:17.166648Z","iopub.execute_input":"2022-10-06T21:23:17.167060Z","iopub.status.idle":"2022-10-06T21:23:17.173821Z","shell.execute_reply.started":"2022-10-06T21:23:17.167021Z","shell.execute_reply":"2022-10-06T21:23:17.172864Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 50000\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:17.175423Z","iopub.execute_input":"2022-10-06T21:23:17.176094Z","iopub.status.idle":"2022-10-06T21:23:17.185734Z","shell.execute_reply.started":"2022-10-06T21:23:17.176059Z","shell.execute_reply":"2022-10-06T21:23:17.184724Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][:2]","metadata":{"id":"IXssc1MjX5v2","outputId":"58d309a5-bf5b-4338-bdd5-491ed8fadcea","execution":{"iopub.status.busy":"2022-10-06T21:23:17.187433Z","iopub.execute_input":"2022-10-06T21:23:17.188284Z","iopub.status.idle":"2022-10-06T21:23:17.196936Z","shell.execute_reply.started":"2022-10-06T21:23:17.188247Z","shell.execute_reply":"2022-10-06T21:23:17.195964Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'],\n 'label': [0, 0]}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pretreatment **(1 point)**\n\nCode the `pretreatment` function which clean the input text. Look at the dataset and deduce which treatment is needed.","metadata":{"id":"5UoMzhBOeGxe"}},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:17.198577Z","iopub.execute_input":"2022-10-06T21:23:17.199252Z","iopub.status.idle":"2022-10-06T21:23:26.456777Z","shell.execute_reply.started":"2022-10-06T21:23:17.199215Z","shell.execute_reply":"2022-10-06T21:23:26.455589Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.12.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#library needed for pretreatment\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:26.459525Z","iopub.execute_input":"2022-10-06T21:23:26.459934Z","iopub.status.idle":"2022-10-06T21:23:26.983304Z","shell.execute_reply.started":"2022-10-06T21:23:26.459889Z","shell.execute_reply":"2022-10-06T21:23:26.982313Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#get stopwords\nnltk.download('stopwords')\nstops = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:26.985262Z","iopub.execute_input":"2022-10-06T21:23:26.985548Z","iopub.status.idle":"2022-10-06T21:23:27.123954Z","shell.execute_reply.started":"2022-10-06T21:23:26.985516Z","shell.execute_reply":"2022-10-06T21:23:27.123021Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def remove_stop_words(text: str) -> str:\n    \"\"\"Remove stopwrods from the string.\n    Args: \n        text: an input string.\n    Returns:\n        The text without stopword.\n    \"\"\"\n    text_words = text.split()\n    text_no_stop = [word for word in text_words if word not in stops]\n    res = ' '.join(text_no_stop)\n    return res\n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:27.125323Z","iopub.execute_input":"2022-10-06T21:23:27.126335Z","iopub.status.idle":"2022-10-06T21:23:27.132682Z","shell.execute_reply.started":"2022-10-06T21:23:27.126296Z","shell.execute_reply":"2022-10-06T21:23:27.131400Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def pretreatment(text: str) -> str:\n    \"\"\"Clean IMDB text entries.\n    Args:\n        text: an input string.\n    Returns:\n        The cleaned text.\n    \"\"\"\n    text = text.lower()\n    for e in '[{}]'.format(punctuation):\n        text = text.replace(e,'')\n    text = remove_stop_words(text)\n    return text\n","metadata":{"id":"Cw3lht4AZHQB","execution":{"iopub.status.busy":"2022-10-06T21:23:27.134465Z","iopub.execute_input":"2022-10-06T21:23:27.134885Z","iopub.status.idle":"2022-10-06T21:23:27.142761Z","shell.execute_reply.started":"2022-10-06T21:23:27.134847Z","shell.execute_reply":"2022-10-06T21:23:27.141890Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# This applies the pretreatment function to all\nclean_dataset = dataset.map(lambda x: {\"text\": pretreatment(x[\"text\"]), \"label\": x[\"label\"]})","metadata":{"id":"AnxsNRj1b4ef","outputId":"4ee291a4-0778-435b-cbde-bd8a16f49e3c","execution":{"iopub.status.busy":"2022-10-06T21:23:27.144958Z","iopub.execute_input":"2022-10-06T21:23:27.145786Z","iopub.status.idle":"2022-10-06T21:23:36.843589Z","shell.execute_reply.started":"2022-10-06T21:23:27.145677Z","shell.execute_reply":"2022-10-06T21:23:36.842683Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4687a1c5c04f44abbe9f3de29d77c121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"520c119023a741bca781176b5d896c89"}},"metadata":{}}]},{"cell_type":"code","source":"clean_dataset[\"train\"][\"text\"][:3]","metadata":{"id":"ss9fNfoWUAz_","outputId":"10989905-6187-4554-a8e4-20c592435557","execution":{"iopub.status.busy":"2022-10-06T21:23:36.848893Z","iopub.execute_input":"2022-10-06T21:23:36.849185Z","iopub.status.idle":"2022-10-06T21:23:36.903485Z","shell.execute_reply.started":"2022-10-06T21:23:36.849156Z","shell.execute_reply":"2022-10-06T21:23:36.902135Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['rented curiousyellow video store controversy surrounded first released 1967 also heard first seized us customs ever tried enter country therefore fan films considered controversial really see myselfbr br plot centered around young swedish drama student named lena wants learn everything life particular wants focus attentions making sort documentary average swede thought certain political issues vietnam war race issues united states asking politicians ordinary denizens stockholm opinions politics sex drama teacher classmates married menbr br kills curiousyellow 40 years ago considered pornographic really sex nudity scenes far even shot like cheaply made porno countrymen mind find shocking reality sex nudity major staple swedish cinema even ingmar bergman arguably answer good old boy john ford sex scenes filmsbr br commend filmmakers fact sex shown film shown artistic purposes rather shock people make money shown pornographic theaters america curiousyellow good film anyone wanting study meat potatoes pun intended swedish cinema really film doesnt much plot',\n 'curious yellow risible pretentious steaming pile doesnt matter ones political views film hardly taken seriously level claim frontal male nudity automatic nc17 isnt true ive seen rrated films male nudity granted offer fleeting views rrated films gaping vulvas flapping labia nowhere dont exist goes crappy cable shows schlongs swinging breeze clitoris sight pretentious indie movies like brown bunny treated site vincent gallos throbbing johnson trace pink visible chloe sevigny crying implying doublestandard matters nudity mentally obtuse take account one unavoidably obvious anatomical difference men women genitals display actresses appears nude cannot said man fact generally wont see female genitals american film anything short porn explicit erotica alleged doublestandard less double standard admittedly depressing ability come terms culturally insides womens bodies',\n 'avoid making type film future film interesting experiment tells cogent storybr br one might feel virtuous sitting thru touches many important issues without discernable motive viewer comes away new perspectives unless one comes one ones mind wanders invariably pointless filmbr br one might better spend ones time staring window tree growingbr br']"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's take a quick look at the labels. Notice that the labels are ordered in the training set starting by the negative reviews (0), followed by the positive ones (1). Training neural networks on this kind of configuration tends to considerably affect their performances. So the dataset will have to be shuffled.","metadata":{"id":"4sAscTs3U2Sn"}},{"cell_type":"code","source":"clean_dataset[\"train\"][\"label\"][12490:12510]","metadata":{"id":"lZedAEQpdstA","outputId":"db3cbcc8-cf62-4035-8605-225249af6e9b","execution":{"iopub.status.busy":"2022-10-06T21:23:36.905269Z","iopub.execute_input":"2022-10-06T21:23:36.905678Z","iopub.status.idle":"2022-10-06T21:23:37.144490Z","shell.execute_reply.started":"2022-10-06T21:23:36.905637Z","shell.execute_reply":"2022-10-06T21:23:37.143376Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train/validation split **(1 point)**\n\nIn our example, we consider the test split as production data. Which means, we need to treat it as if we never see it during the training process. To experiment on the model, we need to split the training set into a training and validation set. See [here](https://huggingface.co/course/chapter5/3?fw=pt#creating-a-validation-set) on how to do so with the `Datasets` library.\n\nDon't forget to **stratify** your split (we need to have the same proportion of class in both training and validation set).","metadata":{"id":"Hv3rt4ZieQbN"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntraining_set, validation_set = train_test_split(clean_dataset[\"train\"], test_size = 0.2)","metadata":{"id":"5QIBkz-0YAO7","execution":{"iopub.status.busy":"2022-10-06T21:23:37.145905Z","iopub.execute_input":"2022-10-06T21:23:37.147272Z","iopub.status.idle":"2022-10-06T21:23:37.331542Z","shell.execute_reply.started":"2022-10-06T21:23:37.147231Z","shell.execute_reply":"2022-10-06T21:23:37.330515Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Creation of the DatasetDict general\nimport datasets\ndataset_dict = datasets.DatasetDict({\n   \"train\": Dataset.from_dict(training_set),\n   \"test\": clean_dataset[\"test\"],\n    \"validation\": Dataset.from_dict(validation_set)\n})","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.333202Z","iopub.execute_input":"2022-10-06T21:23:37.333569Z","iopub.status.idle":"2022-10-06T21:23:37.387242Z","shell.execute_reply.started":"2022-10-06T21:23:37.333531Z","shell.execute_reply":"2022-10-06T21:23:37.386346Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dataset_dict","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.388764Z","iopub.execute_input":"2022-10-06T21:23:37.389157Z","iopub.status.idle":"2022-10-06T21:23:37.395492Z","shell.execute_reply.started":"2022-10-06T21:23:37.389112Z","shell.execute_reply":"2022-10-06T21:23:37.394456Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Check here that the dataset is stratified.","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.396742Z","iopub.execute_input":"2022-10-06T21:23:37.397722Z","iopub.status.idle":"2022-10-06T21:23:37.404741Z","shell.execute_reply.started":"2022-10-06T21:23:37.397681Z","shell.execute_reply":"2022-10-06T21:23:37.403545Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"label_zero = 0\nlabel_one = 0\nfor e in (dataset_dict[\"train\"][\"label\"]):\n    if (e == 1):\n        label_zero +=1\n    else:\n        label_one +=1 \nlabel_zero, label_one","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.406314Z","iopub.execute_input":"2022-10-06T21:23:37.406823Z","iopub.status.idle":"2022-10-06T21:23:37.425815Z","shell.execute_reply.started":"2022-10-06T21:23:37.406787Z","shell.execute_reply":"2022-10-06T21:23:37.424616Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(10003, 9997)"},"metadata":{}}]},{"cell_type":"code","source":"label_zero = 0\nlabel_one = 0\nfor e in (dataset_dict[\"validation\"][\"label\"]):\n    if (e == 1):\n        label_zero +=1\n    else:\n        label_one +=1 \nlabel_zero, label_one","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.427494Z","iopub.execute_input":"2022-10-06T21:23:37.427868Z","iopub.status.idle":"2022-10-06T21:23:37.441073Z","shell.execute_reply.started":"2022-10-06T21:23:37.427832Z","shell.execute_reply":"2022-10-06T21:23:37.440149Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(2497, 2503)"},"metadata":{}}]},{"cell_type":"markdown","source":"The dataset are considered well startified here","metadata":{}},{"cell_type":"markdown","source":"## Categorical encoding of the vocabulary **(2 points)**\n\nWe can't feed word to a neural network. A usual solution is to turn words into categorical data by using one-hot encoding. To avoid an explosion in vocabulary size, we will only keep words which appear more than a certain amount of time.\n\nThe `Vocabulary` class below will do that for us.","metadata":{"id":"0t40J_xte1dz"}},{"cell_type":"code","source":"UNK_TOKEN = \"<UNK>\"\nPAD_TOKEN = \"<PAD>\"\n\n\nclass Vocabulary:\n    \"\"\"Vocabulary manager on a collection.\n    \"\"\"\n    def __init__(self) -> None:\n        \"\"\"No parameters to provide.\n        \"\"\"\n        # Index to word mapping.\n        self.index2word = [PAD_TOKEN, UNK_TOKEN]\n        # Word to index mapping.\n        self.word2index = {value: key for key, value in enumerate(self.index2word)}\n        # Word counter.\n        self.word2count = defaultdict(int)\n\n    def adataset_dict_word(self, word: str) -> None:\n        \"\"\"Increments the count of a word to the vocabulary.\n        Args:\n            word: the word.\n        \"\"\"\n        self.word2count[word] += 1\n        if not word in self.word2index:\n            self.word2index[word] = len(self.index2word)\n            self.index2word.append(word)\n\n    def adataset_dict_text(self, text: str, separator: str =\" \") -> None:\n        \"\"\"Adataset_dict the words given in a text to our vocabulary.\n        Args:\n            text: a sequence of words separated by a given separator.\n            separator: the separator used to split our text (default is \" \").\n        \"\"\"\n        for word in text.split(separator):\n            self.adataset_dict_word(word)\n\n    def get_index(self, word: str) -> int:\n        \"\"\"Returns the index of a given word in our vocabulary.\n        If the word is not in the vocabulary, returns the index for UNK_TOKEN.\n        Args:\n            word: a string.\n        Returns:\n            The corresponding index or the index for UNK_TOKEN.\n        \"\"\"\n        return (\n            self.word2index[word]\n            if word in self.word2index\n            else self.word2index[UNK_TOKEN]\n        )\n\n    def get_word(self, index: int) -> str:\n        \"\"\"Returns the word at a given index in our vocabulary.\n        Args:\n            index: the word position in our vocabulary.\n        Returns:\n            The word corresponding to the given index.\n        \"\"\"\n        return self.index2word[index]\n\n    def get_word_count(self, word: str) -> int:\n        \"\"\"Returns the number of occurences for a given word.\n        Raise a \n        Args:\n            The word.\n        Returns:\n            Its number of measured occurences.\n        \"\"\"\n        return self.word2count[word]\n\n    def get_vocabulary(self) -> List[str]:\n        \"\"\"Returns a copy of the whole vocabulary list.\n        Returns:\n            A list of words.\n        \"\"\"\n        return deepcopy(self.index2word)\n\n    def __len__(self) -> int:\n        \"\"\"len() function.\n        Returns:\n            The number of words in the vocabulary.\n        \"\"\"\n        return len(self.index2word)\n\n    def trim_vocabulary(self, min_occurences: int = 5) -> None:\n        \"\"\"Trim the vocabulary based on the number of occurrences of each words.\n        Note that whole counts of deleted words are adataset_dicted to the UNK_TOKEN counts.\n        Args:\n            min_occurences: the minimum number of occurences for a word to be kept.\n        \"\"\"\n        to_delete = {\n            word for word, count in self.word2count.items() if count < min_occurences\n        }\n        new_word2count = defaultdict(int)\n        for word, count in self.word2count.items():\n            if word not in to_delete:\n                new_word2count[word] = count\n            else:\n                new_word2count[UNK_TOKEN] += count\n        new_index2word = [word for word in self.index2word if word not in to_delete]\n        new_word2index = {word: index for index, word in enumerate(new_index2word)}\n\n        self.word2count = new_word2count\n        self.index2word = new_index2word\n        self.word2index = new_word2index","metadata":{"id":"T2P9A4j8aarj","execution":{"iopub.status.busy":"2022-10-06T21:23:37.442496Z","iopub.execute_input":"2022-10-06T21:23:37.443026Z","iopub.status.idle":"2022-10-06T21:23:37.457529Z","shell.execute_reply.started":"2022-10-06T21:23:37.442969Z","shell.execute_reply":"2022-10-06T21:23:37.456700Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**(1 point)** Get the vocabulary on both the training and validation set using the `Vocabulary` class. Remember, we don't use the test set here as we consider it as proxy production data. The trim it down as you see fit (around 20K words in the vocabulary is a good value).","metadata":{}},{"cell_type":"code","source":"def get_vocabulary_dataset(dataset: dict, size_max: int):\n    \"\"\"Get the vocabulary of the dataset and trim it until the lenght of it is under the size maximum.\n    Args: \n        dataset: the dataset to get the vocabulary from.\n        size_max: the size max of the vocabulary.\n    Returns:\n        The list of words of the vocabulary.\"\"\"\n    vocabulary = Vocabulary()\n    for text in dataset[\"text\"]:\n        vocabulary.adataset_dict_text(text)\n\n    i = 1\n    while (len(vocabulary) > size_max):\n        vocabulary.trim_vocabulary(i)\n        i += 1\n    \n    return vocabulary","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.460590Z","iopub.execute_input":"2022-10-06T21:23:37.461463Z","iopub.status.idle":"2022-10-06T21:23:37.471197Z","shell.execute_reply.started":"2022-10-06T21:23:37.461429Z","shell.execute_reply":"2022-10-06T21:23:37.470248Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"training_vocab = Vocabulary()\ntraining_vocab = get_vocabulary_dataset(dataset_dict[\"train\"],20000)\nlen(training_vocab.get_vocabulary())","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:37.472345Z","iopub.execute_input":"2022-10-06T21:23:37.473303Z","iopub.status.idle":"2022-10-06T21:23:39.002943Z","shell.execute_reply.started":"2022-10-06T21:23:37.473267Z","shell.execute_reply":"2022-10-06T21:23:39.001651Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"19919"},"metadata":{}}]},{"cell_type":"code","source":"validation_vocab = Vocabulary()\nvalidation_vocab = get_vocabulary_dataset(dataset_dict[\"validation\"],20000)\nlen(validation_vocab.get_vocabulary())","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:39.004787Z","iopub.execute_input":"2022-10-06T21:23:39.005235Z","iopub.status.idle":"2022-10-06T21:23:39.423419Z","shell.execute_reply.started":"2022-10-06T21:23:39.005188Z","shell.execute_reply":"2022-10-06T21:23:39.422337Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"18507"},"metadata":{}}]},{"cell_type":"markdown","source":"**(1 point)** Fill the encoding and decoding functions. The encoding function takes a text as input and returns a list IDs corresponding to the index of each word in the vocabulary. The decoding function reverse the process, turning a list of IDs into a text. Make sure the encoding function returns a numpy array.","metadata":{}},{"cell_type":"code","source":"# Encoding and decoding function\nfrom base64 import encode\n\n\nvocab = training_vocab\ndef encode_text(text: str) -> np.ndarray:\n    encoded_text = []\n    for word in text.split():\n        encoded_text.append(vocab.get_index(word))\n    return encoded_text\n\ndef decode_text(encoded_text: np.ndarray) -> str:\n    decoded_text = []\n    for index in encoded_text :\n        decoded_text.append(vocab.get_word(index))\n    return decoded_text","metadata":{"id":"6Mj4AMqYk6xt","execution":{"iopub.status.busy":"2022-10-06T21:23:39.425093Z","iopub.execute_input":"2022-10-06T21:23:39.425576Z","iopub.status.idle":"2022-10-06T21:23:39.432497Z","shell.execute_reply.started":"2022-10-06T21:23:39.425535Z","shell.execute_reply":"2022-10-06T21:23:39.431309Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Apply the encoding function to the entire dataset.\nencoded_dataset = dataset_dict.map(lambda x: {\"text\": encode_text(x[\"text\"]), \"label\": x[\"label\"]})","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:39.434115Z","iopub.execute_input":"2022-10-06T21:23:39.434560Z","iopub.status.idle":"2022-10-06T21:23:49.030182Z","shell.execute_reply.started":"2022-10-06T21:23:39.434520Z","shell.execute_reply":"2022-10-06T21:23:49.029332Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84773151602c4b01978bd998a6eafec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f041140fed664ead8acfa3bebccae6a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad0382f5d8394f60bc938aacd5a96d40"}},"metadata":{}}]},{"cell_type":"markdown","source":"To make sure everything went well, we compare a text before and after encoding and then decoding it. You should see rare words / typos replaced by the `<UNK>` token.","metadata":{"id":"WPUej2whwaXw"}},{"cell_type":"code","source":"dataset_dict[\"train\"][\"text\"][0], decode_text(encoded_dataset[\"train\"][\"text\"][0])","metadata":{"id":"RT8UQgeWliQz","outputId":"57db68fa-f056-4ad5-e7b8-342ed73f4058","execution":{"iopub.status.busy":"2022-10-06T21:23:49.031459Z","iopub.execute_input":"2022-10-06T21:23:49.031805Z","iopub.status.idle":"2022-10-06T21:23:50.164789Z","shell.execute_reply.started":"2022-10-06T21:23:49.031766Z","shell.execute_reply":"2022-10-06T21:23:50.163774Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"('flock really movie wannabe movie wannabe actors including richard gere gave excellent performance one actors truly gives character rest cast acting result pathetic like movie see idea acting hide fact youre acting hell claire dains one shes inappropriate actress character 999 movie looked extremely place everything thing asking stupid questions like really think making silly faces embarrassed acting seriously used like shes romantic movie type dont know picked among actresses lol seeing avril lavigne really made laugh anyway want get feeling throwing movie job wish could vote 5',\n ['flock',\n  'really',\n  'movie',\n  'wannabe',\n  'movie',\n  'wannabe',\n  'actors',\n  'including',\n  'richard',\n  'gere',\n  'gave',\n  'excellent',\n  'performance',\n  'one',\n  'actors',\n  'truly',\n  'gives',\n  'character',\n  'rest',\n  'cast',\n  'acting',\n  'result',\n  'pathetic',\n  'like',\n  'movie',\n  'see',\n  'idea',\n  'acting',\n  'hide',\n  'fact',\n  'youre',\n  'acting',\n  'hell',\n  'claire',\n  '<UNK>',\n  'one',\n  'shes',\n  'inappropriate',\n  'actress',\n  'character',\n  '999',\n  'movie',\n  'looked',\n  'extremely',\n  'place',\n  'everything',\n  'thing',\n  'asking',\n  'stupid',\n  'questions',\n  'like',\n  'really',\n  'think',\n  'making',\n  'silly',\n  'faces',\n  'embarrassed',\n  'acting',\n  'seriously',\n  'used',\n  'like',\n  'shes',\n  'romantic',\n  'movie',\n  'type',\n  'dont',\n  'know',\n  'picked',\n  'among',\n  'actresses',\n  'lol',\n  'seeing',\n  '<UNK>',\n  '<UNK>',\n  'really',\n  'made',\n  'laugh',\n  'anyway',\n  'want',\n  'get',\n  'feeling',\n  'throwing',\n  'movie',\n  'job',\n  'wish',\n  'could',\n  'vote',\n  '5'])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Batch preparation **(1 point)**\n\nTo speed up learning, and take advantage of the GPU architecture, we provide data to the model by batches. Since all line in the same batch need to have the same length, we pad lines to the maximum length of each batch.","metadata":{"id":"FVadYQD2nw52"}},{"cell_type":"code","source":"def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int = 32, pad_right: bool = False) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate randomly ordered batches of data+labels.\n    Args:\n        X: the input data.\n        y: the corresponding labels.\n        batch_size: the size of each batch [32].\n        pad_right: if true, the padataset_dicting is done on the right [False].\n    \"\"\"\n    \n    X, y = shuffle(X, y)\n    n_batches = int(np.ceil(len(y) / batch_size))\n    \n    for i in range(n_batches):\n        \n        end = min((i+1)*batch_size, len(y))\n        \n        X_batch = X[i*batch_size:end]\n        y_batch = y[i*batch_size:end]\n\n        # Padataset_dicting to max ength size within the batch\n        max_len = np.max([len(x) for x in X_batch])\n        for j in range(len(X_batch)):\n            x = X_batch[j]\n            pad = [training_vocab.get_index(PAD_TOKEN)] * (max_len - len(x))\n            X_batch[j] = x+pad if pad_right else pad+x\n\n        X_batch = torch.from_numpy(np.array(X_batch)).long()\n        y_batch = torch.from_numpy(np.array(y_batch)).long()\n\n        # Yielding results, so every time the function is called, it starts again from here.\n        yield X_batch, y_batch","metadata":{"id":"dWV2pzgqa1cD","execution":{"iopub.status.busy":"2022-10-06T21:23:50.166393Z","iopub.execute_input":"2022-10-06T21:23:50.166991Z","iopub.status.idle":"2022-10-06T21:23:50.176421Z","shell.execute_reply.started":"2022-10-06T21:23:50.166952Z","shell.execute_reply":"2022-10-06T21:23:50.175374Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Let's see what the batches look like.","metadata":{}},{"cell_type":"code","source":"for inputs, labels in data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"], pad_right=True):\n    print(\"inputs:\\n\", inputs, \"\\nshape:\\n\", inputs.shape)\n    print(\"labels:\\n\", labels, \"\\nshape:\\n\", labels.shape)\n    break","metadata":{"id":"d1-D1ueJb6IR","outputId":"1c2de069-4138-48ed-9300-86c0d4472603","execution":{"iopub.status.busy":"2022-10-06T21:23:50.178849Z","iopub.execute_input":"2022-10-06T21:23:50.179167Z","iopub.status.idle":"2022-10-06T21:23:51.107923Z","shell.execute_reply.started":"2022-10-06T21:23:50.179140Z","shell.execute_reply":"2022-10-06T21:23:51.106871Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"inputs:\n tensor([[  269,    76,     4,  ...,     0,     0,     0],\n        [  757,     1,  3778,  ...,     0,     0,     0],\n        [ 9963,  8735,   418,  ...,     0,     0,     0],\n        ...,\n        [    1, 10838,   544,  ...,     0,     0,     0],\n        [ 6297,  6615,   352,  ...,     0,     0,     0],\n        [10837, 11004,  9783,  ...,     0,     0,     0]]) \nshape:\n torch.Size([32, 271])\nlabels:\n tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n        1, 1, 1, 1, 1, 0, 0, 0]) \nshape:\n torch.Size([32])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**(1 point)** Question: On which side should we pad the data for our use case and why?","metadata":{}},{"cell_type":"markdown","source":"We should pad the data to the right. Indeed with this padataset_dicting, the model will see in a first time the indexes of the text which conveys the important information and not the succsession of 0 that represents empty slots to fill the sentence to the max lenght.","metadata":{}},{"cell_type":"markdown","source":"## The model **(13 points)**\n\nWe use a simple RNN with a configurable number of layers.","metadata":{"id":"t5uO-cAWodA0"}},{"cell_type":"code","source":"# Before starting, let's set up the device. A GPU if available, else the CPU.\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"xel73Svtcgrf","outputId":"e1f2b332-ccde-4d48-f4c1-bd7bfffcd1f4","execution":{"iopub.status.busy":"2022-10-06T21:23:51.109347Z","iopub.execute_input":"2022-10-06T21:23:51.110297Z","iopub.status.idle":"2022-10-06T21:23:51.222295Z","shell.execute_reply.started":"2022-10-06T21:23:51.110258Z","shell.execute_reply":"2022-10-06T21:23:51.221271Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"class RNN(nn.Module):\n    \"\"\"A simple RNN module with word embeddings..\n    \"\"\"\n    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n        \"\"\"\n        Args:\n            vocab_size: vocabulary size.\n            embed_size: embedataset_dicting dimensions.\n            hidataset_dicten_size: hidataset_dicten layer size.\n            n_layers: the number of layers.\n            n_outputs: the number of output classes.\n        \"\"\"\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.n_outputs = n_outputs\n\n\n        # The word embedataset_dicting layer.\n        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n        # The RNN\n        self.rnn = nn.RNN(\n            input_size = self.embed_size,\n            hidden_size = self.hidden_size,\n            num_layers = self.n_layers,\n            batch_first = True, # Changes the order of dimension to put the batches first.\n        )\n        # A fully connected layer to project the RNN's output to only one output used for classification.\n        self.fc = nn.Linear(self.hidden_size, self.n_outputs)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Function called when the model is called with data as input.\n        Args:\n            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n        Returns:\n            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n        \"\"\"\n        h0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n\n        out = self.embed(X)\n        # out contains the output layer of all words in the sequence.\n        # First dim is batch, second the word in the sequence, third is the vector itself.\n        # The second output value is the last vector of all intermediate layer.\n        # Only use it if you want to access the intermediate layer values of a\n        # multilayer model.\n        out, _ = self.rnn(out, h0)\n        # Getting the last value only.\n        out = out[:, -1, :]\n    \n        # Linear projection.\n        out = self.fc(out)\n\n        return out","metadata":{"id":"yiNVffK_cnxM","execution":{"iopub.status.busy":"2022-10-06T21:23:51.225664Z","iopub.execute_input":"2022-10-06T21:23:51.225947Z","iopub.status.idle":"2022-10-06T21:23:51.236622Z","shell.execute_reply.started":"2022-10-06T21:23:51.225920Z","shell.execute_reply":"2022-10-06T21:23:51.235720Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Note that we do not pass the output through a sigmoid function. This is because pyTorch implements some code optimization within the `BCEWithLogitsLoss` we'll see later.","metadata":{}},{"cell_type":"code","source":"import copy\nfrom copy import deepcopy\n\ndef train(\n    model: nn.Module,\n    criterion: Callable,\n    optimizier: torch.optim.Optimizer,\n    n_epochs: int,\n    train_gen: Callable,\n    valid_gen: Callable,\n) -> Tuple[List[float], List[float]]:\n    \"\"\"Train a model using a batch gradient descent.\n    Args:\n        model: a class inheriting from nn.Module.\n        criterion: a loss criterion.\n        optimizer: an optimizer (e.g. Adam, RMSprop, ...).\n        n_epochs: the number of training epochs.\n        train_gen: a callable function returing a batch (data, labels).\n        valid_gen: a callable function returing a batch (data, labels).\n    \"\"\"\n    train_losses = np.zeros(n_epochs)\n    valid_losses = np.zeros(n_epochs)\n    model_to_save = None\n    lowest_loss = 0\n\n    for epoch in range(n_epochs):\n\n        t0 = datetime.now()\n        model.train()\n        train_loss = []\n\n        # Training loop.\n        for inputs, labels in train_gen():\n            # labels are of dimension (N,) we turn them into (N, 1).\n            labels = labels.view(-1, 1).float()\n            # Put them on the GPU.\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Reset the gradient.\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizier.step()\n\n            train_loss.append(loss.item())  # .item() detach the value from GPU.\n\n        train_losses[epoch] = np.mean(train_loss)\n\n        model.eval()\n        valid_loss = []\n        # Evaluation loop.\n        for inputs, labels in valid_gen():\n            labels = labels.view(-1, 1).float()\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            valid_loss.append(loss.item())\n\n        valid_losses[epoch] = np.mean(valid_loss)\n        \n        ### CODE MODIFIED TO STOP THE TRAINING IF OVERFITTING\n        \n        if (valid_losses[epoch] < lowest_loss) or (lowest_loss == 0):\n            lowest_loss = valid_losses[epoch]\n            model_to_save = copy.deepcopy(model)\n\n        print(f\"Epoch: {epoch}, training loss: {train_losses[epoch]}, validation loss: {valid_losses[epoch]}, in {datetime.now() - t0}\")\n\n    return train_losses, valid_losses, model_to_save","metadata":{"id":"Zw-lzgfveXB3","execution":{"iopub.status.busy":"2022-10-06T21:23:51.238198Z","iopub.execute_input":"2022-10-06T21:23:51.238651Z","iopub.status.idle":"2022-10-06T21:23:51.252549Z","shell.execute_reply.started":"2022-10-06T21:23:51.238594Z","shell.execute_reply":"2022-10-06T21:23:51.251643Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"We setup the model, criterion (a binary cross entropy), and the optimizer (Adam).\n\nNote that `BCEWithLogitsLoss` use a mathematical trick to incorporate the sigmoid function in its computation. This trick makes the learning process go slightly faster and is the reason why we didn't put a sigmoid in the forward function of the model.","metadata":{}},{"cell_type":"code","source":"model = RNN(len(training_vocab), 32, 64, 1, 1).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"id":"K7Mpe44Dd85Z","execution":{"iopub.status.busy":"2022-10-06T21:23:51.254928Z","iopub.execute_input":"2022-10-06T21:23:51.255294Z","iopub.status.idle":"2022-10-06T21:23:55.321675Z","shell.execute_reply.started":"2022-10-06T21:23:51.255259Z","shell.execute_reply":"2022-10-06T21:23:55.320706Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"We get the 3 generators.","metadata":{}},{"cell_type":"code","source":"train_gen = lambda: data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"])\nvalid_gen = lambda: data_generator(encoded_dataset[\"validation\"][\"text\"], encoded_dataset[\"validation\"][\"label\"])\ntest_gen = lambda: data_generator(encoded_dataset[\"test\"][\"text\"], encoded_dataset[\"test\"][\"label\"])","metadata":{"id":"srYPmX_aeLwD","execution":{"iopub.status.busy":"2022-10-06T21:23:55.322982Z","iopub.execute_input":"2022-10-06T21:23:55.323365Z","iopub.status.idle":"2022-10-06T21:23:55.329695Z","shell.execute_reply.started":"2022-10-06T21:23:55.323327Z","shell.execute_reply":"2022-10-06T21:23:55.328748Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"encoded_dataset","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:23:55.331084Z","iopub.execute_input":"2022-10-06T21:23:55.331982Z","iopub.status.idle":"2022-10-06T21:23:55.343435Z","shell.execute_reply.started":"2022-10-06T21:23:55.331943Z","shell.execute_reply":"2022-10-06T21:23:55.342560Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"And train the model.","metadata":{}},{"cell_type":"code","source":"train_losses, valid_losses, model = train(model, criterion, optimizer, 10, train_gen, valid_gen)","metadata":{"id":"WB98HqD3fk1G","outputId":"ebb04ad2-a669-4462-8df4-87f057c453ad","execution":{"iopub.status.busy":"2022-10-06T21:23:55.346086Z","iopub.execute_input":"2022-10-06T21:23:55.347044Z","iopub.status.idle":"2022-10-06T21:25:46.001164Z","shell.execute_reply.started":"2022-10-06T21:23:55.346989Z","shell.execute_reply":"2022-10-06T21:25:46.000074Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch: 0, training loss: 0.6762918057441711, validation loss: 0.6617666809422196, in 0:00:12.283113\nEpoch: 1, training loss: 0.5956799233913421, validation loss: 0.6511348091113339, in 0:00:10.971826\nEpoch: 2, training loss: 0.5417112821102142, validation loss: 0.5155054465600639, in 0:00:10.785705\nEpoch: 3, training loss: 0.4259020669221878, validation loss: 0.45566746554556925, in 0:00:10.790416\nEpoch: 4, training loss: 0.3701060254335403, validation loss: 0.45481234390264863, in 0:00:11.129887\nEpoch: 5, training loss: 0.46372013520002364, validation loss: 0.566658128598693, in 0:00:10.840268\nEpoch: 6, training loss: 0.3855380420923233, validation loss: 0.44963209122229536, in 0:00:10.876470\nEpoch: 7, training loss: 0.34598712483644484, validation loss: 0.6380320616588471, in 0:00:11.173702\nEpoch: 8, training loss: 0.3862461616277695, validation loss: 0.4658047909949236, in 0:00:10.786664\nEpoch: 9, training loss: 0.2616734348356724, validation loss: 0.4787764188590323, in 0:00:11.009179\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can look at the training and validation loss.","metadata":{}},{"cell_type":"code","source":"plt.plot(train_losses, label=\"Training loss\")\nplt.plot(valid_losses, label=\"Validation loss\")\nplt.legend()","metadata":{"id":"VATw05GYfwNm","outputId":"f732409c-f955-4eff-9f39-fafc94fa7e83","execution":{"iopub.status.busy":"2022-10-06T21:25:46.002818Z","iopub.execute_input":"2022-10-06T21:25:46.003496Z","iopub.status.idle":"2022-10-06T21:25:46.248876Z","shell.execute_reply.started":"2022-10-06T21:25:46.003442Z","shell.execute_reply":"2022-10-06T21:25:46.247962Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<matplotlib.legend.Legend at 0x7f67fb398e10>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"For the assignment, code the following.\n* **(2 points)** The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the `train` function so it returns the model found with the best validation loss.\n* **(2 points)** Adataset_dict an accuracy function and report the accuracy of the training and test set.\n* **(3 points)** Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.\n* **(2 point)** Implement a function which takes any text and return the model's prediction.\n    * The function should have a string as input and return a class (0 or 1) and its probability (score out of a [sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html)).\n    * Don't forget to make the text go through the same pretreatment and encoding you used to train your model.\n* **(3 points)** Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.\n    * To combine the last output of both direction, you can concatenate, adataset_dict, or max-pool them. Please document your choice.\n* **(1 point)** With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.\n* **(Bonus)** Try finding better hyperparameters (dimensions, number of layers, ...). Document your experiments and results.","metadata":{}},{"cell_type":"markdown","source":"(2 points) The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the train function so it returns the model found with the best validation loss.\n\nSee the code above.","metadata":{}},{"cell_type":"markdown","source":"(2 points) Adataset_dict an accuracy function and report the accuracy of the training and test set.","metadata":{}},{"cell_type":"code","source":"def get_accuracy(model, data_gen):\n    \"\"\"Returns the accuracy value of the model\n    Args:\n    model: the model.\n    data_gen: the dataset on which we calculate the accuracy.\n    \n    Returns: The accuracy score.\n    \"\"\"\n    for inputs, labels in data_gen():\n        labels = labels.view(-1,1).float()\n        inputs = inputs.to(device)\n        lables = labels.to(device)\n        \n        outputs = model(inputs)\n        outputs = torch.sigmoid(outputs)\n        outputs = outputs.to(device)\n        \n        tmp = []\n        for e in outputs:\n            if (e.item() >= 0.5):\n                tmp.append(1)\n            else :\n                tmp.append(0)\n        \n        correct_prediction = 0\n        number_of_prediction = 0\n        \n        for i in range(len(inputs)):\n            if (labels[i].to(device) == tmp[i]):\n                correct_prediction += 1\n            number_of_prediction += 1\n    accuracy = correct_prediction / number_of_prediction\n    print(f\"The accuracy of the model is {accuracy} \\n\")\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:25:46.251085Z","iopub.execute_input":"2022-10-06T21:25:46.251760Z","iopub.status.idle":"2022-10-06T21:25:46.259777Z","shell.execute_reply.started":"2022-10-06T21:25:46.251718Z","shell.execute_reply":"2022-10-06T21:25:46.258908Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"get_accuracy(model,test_gen)\nget_accuracy(model,train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:25:46.261313Z","iopub.execute_input":"2022-10-06T21:25:46.261697Z","iopub.status.idle":"2022-10-06T21:26:00.185742Z","shell.execute_reply.started":"2022-10-06T21:25:46.261647Z","shell.execute_reply":"2022-10-06T21:26:00.184701Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:473: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:926.)\n  self.batch_first)\n","output_type":"stream"},{"name":"stdout","text":"The accuracy of the model is 0.875 \n\nThe accuracy of the model is 0.875 \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"(3 points) Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        return self.fc(output[:,-1,:])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:00.192120Z","iopub.execute_input":"2022-10-06T21:26:00.193253Z","iopub.status.idle":"2022-10-06T21:26:00.200038Z","shell.execute_reply.started":"2022-10-06T21:26:00.193212Z","shell.execute_reply":"2022-10-06T21:26:00.199050Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model_LSTM = LSTM(len(training_vocab), 100, 128, 1, 2).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model_LSTM.parameters())","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:00.203018Z","iopub.execute_input":"2022-10-06T21:26:00.203310Z","iopub.status.idle":"2022-10-06T21:26:00.234716Z","shell.execute_reply.started":"2022-10-06T21:26:00.203271Z","shell.execute_reply":"2022-10-06T21:26:00.233733Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"get_accuracy(model_LSTM,test_gen)\nget_accuracy(model_LSTM,train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:00.236170Z","iopub.execute_input":"2022-10-06T21:26:00.236780Z","iopub.status.idle":"2022-10-06T21:26:26.344504Z","shell.execute_reply.started":"2022-10-06T21:26:00.236744Z","shell.execute_reply":"2022-10-06T21:26:26.343251Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"The accuracy of the model is 0.5 \n\nThe accuracy of the model is 0.40625 \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"(2 point) Implement a function which takes any text and return the model's prediction.\n\n    -The function should have a string as input and return a class (0 or 1) and its probability (score out of a sigmoid).\n    -Don't forget to make the text go through the same pretreatment and encoding you used to train your model.","metadata":{}},{"cell_type":"code","source":"def predict(text, model):\n    \"\"\"Predict the result of the sentence, 1 if positive 0 if negative.\n    Args:\n    text : the sentence to predict.\n    model: the model.\n    \n    Returns: 1 or 0\n    \"\"\"\n    #To fix the parameters\n    model.eval()\n    text = pretreatment(text)\n    # The text is encoded to\n    text = torch.tensor(encode_text(text)).view(1, -1).to(device)\n    output = model(text)\n    output = torch.sigmoid(output)\n    output = (output > 0.5).float()\n    return output.item()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:26.346407Z","iopub.execute_input":"2022-10-06T21:26:26.346824Z","iopub.status.idle":"2022-10-06T21:26:26.355375Z","shell.execute_reply.started":"2022-10-06T21:26:26.346786Z","shell.execute_reply":"2022-10-06T21:26:26.354070Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#test de la prediction\n\npredict(\"I liked this movie, the soundtrack is impressive, the actors play very well!\",model)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:26.356885Z","iopub.execute_input":"2022-10-06T21:26:26.358069Z","iopub.status.idle":"2022-10-06T21:26:26.380812Z","shell.execute_reply.started":"2022-10-06T21:26:26.358026Z","shell.execute_reply":"2022-10-06T21:26:26.379914Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}]},{"cell_type":"code","source":"#test de la prediction\npredict(\"Film to flee, waste of time and money, the actors are not at all credible in their roles\",model)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:26:26.383035Z","iopub.execute_input":"2022-10-06T21:26:26.383660Z","iopub.status.idle":"2022-10-06T21:26:26.393143Z","shell.execute_reply.started":"2022-10-06T21:26:26.383620Z","shell.execute_reply":"2022-10-06T21:26:26.391806Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}]},{"cell_type":"markdown","source":"(3 points) Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.","metadata":{}},{"cell_type":"code","source":"class BiLSTM(nn.Module):\n    def init(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n        super().init()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True ,batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, text):\n        embedataset_dicted = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        return self.fc(output[:,-1,:])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:45:33.474019Z","iopub.execute_input":"2022-10-06T21:45:33.474395Z","iopub.status.idle":"2022-10-06T21:45:33.481502Z","shell.execute_reply.started":"2022-10-06T21:45:33.474363Z","shell.execute_reply":"2022-10-06T21:45:33.480534Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"(1 point) With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.","metadata":{}},{"cell_type":"code","source":"def get_index(prediction):\n    \"\"\"Description\n    Args:\n        prediction: Series containing a binary prediction\n    Return:\n        res: List of elements that were not guessed correctly\n    \"\"\"\n    columns = ['pred','label']\n    df = pd.DataFrame(list(zip(prediction,dataset_dict[\"test\"][\"label\"])),columns=columns)\n    res = []\n    for index,row in df.iterrows():\n        if row[\"pred\"] != row[\"label\"]:\n            res.append(index)\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:35:53.319042Z","iopub.execute_input":"2022-10-06T21:35:53.319437Z","iopub.status.idle":"2022-10-06T21:35:53.326169Z","shell.execute_reply.started":"2022-10-06T21:35:53.319405Z","shell.execute_reply":"2022-10-06T21:35:53.325060Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"dataset_dict[\"test\"] = dataset_dict[\"test\"].add_column(\"prediction\", [predict(i,model) for i in dataset_dict[\"test\"][\"text\"]])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:34:11.352696Z","iopub.execute_input":"2022-10-06T21:34:11.353236Z","iopub.status.idle":"2022-10-06T21:35:01.824894Z","shell.execute_reply.started":"2022-10-06T21:34:11.353189Z","shell.execute_reply":"2022-10-06T21:35:01.823917Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"dataset_dict","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:35:06.263023Z","iopub.execute_input":"2022-10-06T21:35:06.263397Z","iopub.status.idle":"2022-10-06T21:35:06.270286Z","shell.execute_reply.started":"2022-10-06T21:35:06.263365Z","shell.execute_reply":"2022-10-06T21:35:06.269352Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'prediction'],\n        num_rows: 25000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"incorrect = get_index(dataset_dict[\"test\"][\"prediction\"])\nfor i in range(5):\n    if i<len(incorrect):\n        print(dataset_dict[\"test\"][\"text\"][incorrect[i]])\n        print(\"\\n ------------------------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T21:36:30.237428Z","iopub.execute_input":"2022-10-06T21:36:30.237835Z","iopub.status.idle":"2022-10-06T21:36:31.505362Z","shell.execute_reply.started":"2022-10-06T21:36:30.237802Z","shell.execute_reply":"2022-10-06T21:36:31.504333Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"star rating saturday night friday night friday morning sunday night monday morning br br former new orleans homicide cop jack robideaux jean claude van damme reassigned columbus small violent town mexico help police efforts stop major heroin smuggling operation town culprits turn exmilitary lead former commander benjamin meyers stephen lord otherwise known jase east enders using special method learned afghanistan fight opponents jack personal reason taking draws two men explosive final showdown one walk away alivebr br death van damme appeared high showing could make best straight video films action market far drama oriented film shepherd returned highkicking brainer action first made famous sadly produced worst film since derailed nowhere near bad film said still standsbr br dull predictable film little way exciting action little mainly consists limp fight scenes trying look cool trendy cheap slomosped effects added sadly instead make look desperate mexican set film director isaac florentine tried give film robert rodriguezdesperado sort feel adds desperationbr br vd gives particularly uninspired performance given hes never robert de niro sort actor cant good villain lord shouldnt expect leave beeb anytime soon gets little dialogue beginning struggles muster american accent gets mysteriously better towards end supporting cast equally bland nothing raise films spirits allbr br one shepherd thats strayed right flock\n\n ------------------------------\n\nfirst let say havent enjoyed van damme movie since bloodsport probably like movie movies may best plots best actors enjoy kinds movies movie much better movies action guys segal dolph thought putting past years van damme good movie movie worth watching van damme fans good wake death highly recommend anyone likes van damme hell opinion worth watching type feel nowhere run good fun stuff\n\n ------------------------------\n\nblind date columbia pictures 1934 decent film issues film first dont fault actors film less problem script also understand film made 1930s people looking escape reality script made ann sotherns character look weak kept going back forth suitors felt though stayed paul kellys character end truly care family would done anything giving end fickle neil hamilton opinion good time paul kellys character although workaholic man integrity truly loved kitty ann sothern opposed neil hamilton like lot didnt see depth love character production values great script could used little work\n\n ------------------------------\n\nlove italian horror films cheesier better however cheesy italian weekold spaghetti sauce rotting meatballs amateur hour every level suspense horror drops blood scattered around remind fact watching horror film special effects consist lights changing red whenever ghost whatever supposed around string pulling bed sheets oooh feel chills dvd quality vhs transfer actually helps film hurts dubbing even lowest bad italian movie standards gave one star dialogue hilarious discover finally look attic scene daytime one minute night nextwell wont spoil anyone really wants see lets say isnt novel\n\n ------------------------------\n\nben rupert grint deeply unhappy adolescent son unhappily married parents father nicholas farrell vicar mother laura linney well lets say shes somewhat hypocritical soldier jesus army takes summer job assistant foulmouthed eccentric oncefamous nowforgotten actress evie walton julie walters finally finds true harold maude fashion course evie deeply unhappy two sad sacks find put mutual misery aside hit road happinessbr br course corny sentimental predictable hard side walters could sleepwalk way sort thing wanted excellent puts craziness one side finds pathos character like hitting bottle throwing sink shes best problem shes interesting character film script doesnt anybody favours grint hand isnt unhappy hes bit bore well linneys starched bitch completely onedimensional still shes got english accent pat best said mildly enjoyable emphasis mildly\n\n ------------------------------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The reason why the model did not classify well these examples is because the autors are using negative words in their sentences but to give a good opinion of the movie. Then it become harder for the model to classify them into the goood label.","metadata":{}}]}